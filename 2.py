import streamlit as st
import pandas as pd
import plotly.express as px
import os

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="ì„œìš¸ ëŒ€ì¤‘êµí†µ ë°ì´í„° ë¶„ì„")

# --- 2. [í•„ìˆ˜] íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
BASE_PATH = "/Users/kil07201/Desktop/py"

# --- 3. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---

# [ìˆ˜ì •] ìºì‹œ ë°ì½”ë ˆì´í„° ì¶”ê°€: ì´ í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
@st.cache_data
def load_congestion_data(file_name):
    """
    (CSV ë¡œë“œ) í˜¼ì¡ë„ CSVë¥¼ ì½ê³ , 'melt' ì²˜ë¦¬
    """
    data = None
    full_path = os.path.join(BASE_PATH, file_name)
    try:
        data = pd.read_csv(full_path, encoding='utf-8')
    except UnicodeDecodeError:
        data = pd.read_csv(full_path, encoding='cp949') 
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
        return None
    except Exception as e:
        st.error(f"ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

    try:
        if not data['í˜¸ì„ '].astype(str).str.contains('í˜¸ì„ ').all():
             data['í˜¸ì„ '] = data['í˜¸ì„ '].astype(str) + 'í˜¸ì„ '
    except KeyError:
        st.error("í˜¼ì¡ë„ ë°ì´í„°ì— 'í˜¸ì„ ' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    try:
        id_vars = data.columns[:5]  
        time_vars = data.columns[5:] 
        data_long = pd.melt(data, id_vars=id_vars, value_vars=time_vars, 
                            var_name='ì‹œê°„', value_name='í˜¼ì¡ë„')
        data_long['í˜¼ì¡ë„'] = pd.to_numeric(data_long['í˜¼ì¡ë„'], errors='coerce')
        data_long['ì‹œê°„'] = pd.Categorical(data_long['ì‹œê°„'], categories=time_vars, ordered=True)
        return data_long
    except Exception as e:
        st.error(f"í˜¼ì¡ë„ ë°ì´í„° ë³€í™˜(melt) ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# [ìˆ˜ì •] ìºì‹œ ë°ì½”ë ˆì´í„° ì¶”ê°€
@st.cache_data
def load_passenger_data(file_name):
    """
    (CSV ë¡œë“œ) ìŠ¹í•˜ì°¨ ì¸ì› CSVë¥¼ ì½ê³ , 'ìš”ì¼ë³„ í‰ê· 'ê¹Œì§€ ê³„ì‚°
    """
    data = None
    full_path = os.path.join(BASE_PATH, file_name) 
    try:
        data = pd.read_csv(full_path, encoding='utf-8')
    except UnicodeDecodeError:
        data = pd.read_csv(full_path, encoding='cp949')
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
        return None
    except Exception as e:
        st.error(f"ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

    try:
        data['ìˆ˜ì†¡ì¼ì'] = pd.to_datetime(data['ìˆ˜ì†¡ì¼ì'])
        weekday_map = {0: 'í‰ì¼', 1: 'í‰ì¼', 2: 'í‰ì¼', 3: 'í‰ì¼', 4: 'í‰ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'}
        data['ìš”ì¼êµ¬ë¶„'] = data['ìˆ˜ì†¡ì¼ì'].dt.weekday.map(weekday_map)
         
        data.rename(columns={'ì—­ëª…': 'ì¶œë°œì—­'}, inplace=True) 
        if not data['í˜¸ì„ '].astype(str).str.contains('í˜¸ì„ ').all():
             data['í˜¸ì„ '] = data['í˜¸ì„ '].astype(str) + 'í˜¸ì„ '

        group_by_cols = ['ìš”ì¼êµ¬ë¶„', 'í˜¸ì„ ', 'ì¶œë°œì—­', 'ìŠ¹í•˜ì°¨êµ¬ë¶„']
        time_cols = data.columns[6:-1] 
        id_vars = group_by_cols
        data_long = pd.melt(data, 
                            id_vars=id_vars, 
                            value_vars=time_cols, 
                            var_name='ì‹œê°„ëŒ€', 
                            value_name='ì¸ì›ìˆ˜')
         
        data_long['ì¸ì›ìˆ˜'] = pd.to_numeric(data_long['ì¸ì›ìˆ˜'], errors='coerce')

        grouped_data = data_long.groupby(
            ['ìš”ì¼êµ¬ë¶„', 'í˜¸ì„ ', 'ì¶œë°œì—­', 'ìŠ¹í•˜ì°¨êµ¬ë¶„', 'ì‹œê°„ëŒ€']
        )['ì¸ì›ìˆ˜'].mean().reset_index()
         
        grouped_data['ì‹œê°„ëŒ€'] = pd.Categorical(grouped_data['ì‹œê°„ëŒ€'], categories=time_cols, ordered=True)
        grouped_data = grouped_data.sort_values('ì‹œê°„ëŒ€')
         
        return grouped_data
     
    except Exception as e:
        st.error(f"ìŠ¹í•˜ì°¨ ì¸ì› ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# [ìˆ˜ì •] ìºì‹œ ë°ì½”ë ˆì´í„° ì¶”ê°€
@st.cache_data
def load_ranking_data(file_name):
    """
    (CSV ë¡œë“œ) ìˆ˜ì†¡ìˆœìœ„ CSVë¥¼ ì½ëŠ” í•¨ìˆ˜ (ë‹¨ìˆœ ë¡œë“œ)
    """
    data = None
    full_path = os.path.join(BASE_PATH, file_name) 
    try:
        data = pd.read_csv(full_path, encoding='utf-8')
    except UnicodeDecodeError:
        data = pd.read_csv(full_path, encoding='cp949')
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
        return None
    except Exception as e:
        st.error(f"ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None
         
    try:
        if 'í˜¸ì„ ' in data.columns:
            if not data['í˜¸ì„ '].astype(str).str.contains('í˜¸ì„ ').all():
                 data['í˜¸ì„ '] = data['í˜¸ì„ '].astype(str) + 'í˜¸ì„ '
        return data
    except Exception as e:
        st.error(f"ìˆ˜ì†¡ ìˆœìœ„ ë°ì´í„° 'í˜¸ì„ ' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return data

# [ìˆ˜ì •] ìºì‹œ ë°ì½”ë ˆì´í„° ì¶”ê°€
@st.cache_data
def load_bus_data(location_file, status_file):
    """
    ë²„ìŠ¤ ìœ„ì¹˜(Excel), í˜„í™©(Excel íƒ­) 2ê°œ íŒŒì¼ì„ ì½ê³  í•©ì¹©ë‹ˆë‹¤.
    """
    data_location = None
    data_status = None
     
    # 1. ìœ„ì¹˜ íŒŒì¼ ë¡œë“œ (Excel)
    full_path_loc = os.path.join(BASE_PATH, location_file) 
    try:
        data_location = pd.read_excel(full_path_loc, engine='openpyxl', sheet_name='Data')
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path_loc}")
        return None
    except Exception as e:
        st.error(f"ë²„ìŠ¤ ìœ„ì¹˜(Excel) ë¡œë“œ ì˜¤ë¥˜: {e}")
        st.warning("íŒ: openpyxl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. (pip install openpyxl)")
        return None

    # 2. í˜„í™© íŒŒì¼ ë¡œë“œ (Excelì˜ íŠ¹ì • íƒ­)
    full_path_stat = os.path.join(BASE_PATH, status_file)
    try:
        sheet_name_to_load = '2023ë…„ 12ì›” 4ì¼ ê¸°ì¤€'
        data_status = pd.read_excel(full_path_stat, engine='openpyxl', sheet_name=sheet_name_to_load)
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path_stat}")
        return None
    except Exception as e:
        st.error(f"ë²„ìŠ¤ í˜„í™©(Excel) ë¡œë“œ ì˜¤ë¥˜: {e}")
        st.warning(f"íŒ: '{status_file}' íŒŒì¼ ì•ˆì— '{sheet_name_to_load}' íƒ­ì´ ì •í™•íˆ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None
         
    # 3. 'ì •ë¥˜ì†Œëª…'ì„ ê¸°ì¤€ìœ¼ë¡œ ë‘ ë°ì´í„° í•©ì¹˜ê¸° (merge)
    try:
        if 'ì •ë¥˜ì†Œëª…' not in data_location.columns or 'NODE_ID' not in data_location.columns:
            st.error(f"{location_file}ì˜ 'Data' íƒ­ì— 'ì •ë¥˜ì†Œëª…' ë˜ëŠ” 'NODE_ID' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        if 'ì •ë¥˜ì†Œëª…' not in data_status.columns or 'ë…¸ì„ ìˆ˜' not in data_status.columns:
            st.error(f"{status_file}ì˜ '{sheet_name_to_load}' íƒ­ì— 'ì •ë¥˜ì†Œëª…' ë˜ëŠ” 'ë…¸ì„ ìˆ˜' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        location_cols = ['ì •ë¥˜ì†Œëª…', 'NODE_ID']
        status_cols = ['ì •ë¥˜ì†Œëª…', 'ë…¸ì„ ìˆ˜']
         
        merged_bus_data = pd.merge(
            data_location[location_cols], 
            data_status[status_cols], 
            on='ì •ë¥˜ì†Œëª…', 
            how='inner'
        )
         
        merged_bus_data = merged_bus_data.drop_duplicates('NODE_ID')
        merged_bus_data.rename(columns={'NODE_ID': 'ì •ë¥˜ì†Œë²ˆí˜¸'}, inplace=True)
         
        return merged_bus_data
         
    except Exception as e:
        st.error(f"ë²„ìŠ¤ ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
        return None


# --- 4. í˜ì´ì§€ í™”ë©´ í•¨ìˆ˜ ---

def show_home(count, total_files):
    """í™ˆ í™”ë©´"""
    st.header("ğŸš‡ ì„œìš¸ ëŒ€ì¤‘êµí†µ ë°ì´í„° ë¶„ì„")
    st.text("ì‚¬ì´ë“œë°” ë©”ë‰´ì—ì„œ ë¶„ì„ ë‚´ìš©ì„ ì„ íƒí•˜ì„¸ìš”.")
    st.image("https://mediahub.seoul.go.kr/uploads/mediahub/2024/09/jXjYOLlbMGtMRfhWswMBpgzNqagnuOrd.jpg")
    st.success(f"ì´ {total_files}ê°œì˜ ë°ì´í„° ì¤‘ {count}ê°œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")


def show_congestion_analysis(data_congestion, data_passenger):
    """í˜¼ì¡ë„ ë¶„ì„ í˜ì´ì§€"""
    st.header("ğŸ“ˆ ì§€í•˜ì²  í˜¼ì¡ë„(%) ë¶„ì„")

    # [ìˆ˜ì •] set ì—°ì‚°ì„ ì‚¬ìš©í•´ ê³µí†µ í•­ëª© ê³„ì‚°ì„ ë” ë¹ ë¥´ê²Œ í•©ë‹ˆë‹¤.
    days_in_congestion = set(data_congestion['ìš”ì¼êµ¬ë¶„'].unique())
    days_in_passenger = set(data_passenger['ìš”ì¼êµ¬ë¶„'].unique())
    common_days = sorted(list(days_in_congestion & days_in_passenger))

    if not common_days:
        st.error("ë¶„ì„í•  ê³µí†µ ìš”ì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    lines_in_congestion = set(data_congestion['í˜¸ì„ '].unique())
    lines_in_passenger = set(data_passenger['í˜¸ì„ '].unique())
    common_lines = sorted(list(lines_in_congestion & lines_in_passenger))
    
    if not common_lines:
        st.error("ë¶„ì„í•  ê³µí†µ í˜¸ì„  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    col1, col2 = st.columns(2)
    with col1:
        selected_day = st.selectbox("ìš”ì¼ ì„ íƒ", common_days, key="cong_day")
    with col2:
        selected_line = st.selectbox("í˜¸ì„  ì„ íƒ", common_lines, key="cong_line")
     
    directions = sorted(data_congestion[
        (data_congestion['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_congestion['í˜¸ì„ ']==selected_line)
    ]['ìƒí•˜êµ¬ë¶„'].unique())
    selected_dir = st.selectbox("ë°©í–¥ ì„ íƒ", directions, key="cong_dir")

    stations_in_congestion = set(data_congestion[
        (data_congestion['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_congestion['í˜¸ì„ ']==selected_line)
    ]['ì¶œë°œì—­'].unique())
    
    stations_in_passenger = set(data_passenger[
        (data_passenger['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_passenger['í˜¸ì„ ']==selected_line)
    ]['ì¶œë°œì—­'].unique())
    
    common_stations = sorted(list(stations_in_congestion & stations_in_passenger))
     
    if not common_stations:
        st.warning("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ê³µí†µ ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_stations = st.multiselect("ì—­ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", common_stations, default=common_stations[0], key="cong_station")
    if not selected_stations:
        st.warning("ì—­ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return

    filtered = data_congestion[
        (data_congestion['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_congestion['í˜¸ì„ ']==selected_line) &
        (data_congestion['ìƒí•˜êµ¬ë¶„']==selected_dir) &
        (data_congestion['ì¶œë°œì—­'].isin(selected_stations))
    ].copy()
    filtered['í˜¼ì¡ë„'] = filtered['í˜¼ì¡ë„'].fillna(0) 

    if filtered.empty:
        st.warning("ì„ íƒí•œ ì¡°ê±´ì˜ í˜¼ì¡ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    fig = px.line(filtered, x='ì‹œê°„', y='í˜¼ì¡ë„', color='ì¶œë°œì—­', markers=True,
                  title=f"{selected_day} / {selected_line} / {selected_dir} ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë„")
    fig.update_layout(xaxis_title="ì‹œê°„", yaxis_title="í˜¼ì¡ë„ (%)", xaxis={'type': 'category'})
    st.plotly_chart(fig, use_container_width=True)
    st.dataframe(filtered)


def show_passenger_analysis(data_congestion, data_passenger):
    """ìŠ¹í•˜ì°¨ ì¸ì› ë¶„ì„ í˜ì´ì§€"""
    st.header("ğŸ‘¥ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ì¸ì›(ëª…) ë¶„ì„")
     
    # [ìˆ˜ì •] set ì—°ì‚°ì„ ì‚¬ìš©í•´ ê³µí†µ í•­ëª© ê³„ì‚°ì„ ë” ë¹ ë¥´ê²Œ í•©ë‹ˆë‹¤.
    days_in_congestion = set(data_congestion['ìš”ì¼êµ¬ë¶„'].unique())
    days_in_passenger = set(data_passenger['ìš”ì¼êµ¬ë¶„'].unique())
    common_days = sorted(list(days_in_congestion & days_in_passenger))

    if not common_days:
        st.error("ë¶„ì„í•  ê³µí†µ ìš”ì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    lines_in_congestion = set(data_congestion['í˜¸ì„ '].unique())
    lines_in_passenger = set(data_passenger['í˜¸ì„ '].unique())
    common_lines = sorted(list(lines_in_congestion & lines_in_passenger))
    
    if not common_lines:
        st.error("ë¶„ì„í•  ê³µí†µ í˜¸ì„  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    col1, col2 = st.columns(2)
    with col1:
        selected_day = st.selectbox("ìš”ì¼ ì„ íƒ", common_days, key="pass_day")
    with col2:
        selected_line = st.selectbox("í˜¸ì„  ì„ íƒ", common_lines, key="pass_line")
     
    ride_types = sorted(data_passenger['ìŠ¹í•˜ì°¨êµ¬ë¶„'].unique())
    selected_ride = st.radio("ìŠ¹í•˜ì°¨ êµ¬ë¶„", ride_types, horizontal=True, key="pass_type")
     
    stations_in_congestion = set(data_congestion[
        (data_congestion['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_congestion['í˜¸ì„ ']==selected_line)
    ]['ì¶œë°œì—­'].unique())
    
    stations_in_passenger = set(data_passenger[
        (data_passenger['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_passenger['í˜¸ì„ ']==selected_line)
    ]['ì¶œë°œì—­'].unique())
    
    common_stations = sorted(list(stations_in_congestion & stations_in_passenger))
     
    if not common_stations:
        st.warning("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ê³µí†µ ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_stations = st.multiselect("ì—­ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", common_stations, default=common_stations[0], key="pass_station")
    if not selected_stations:
        st.warning("ì—­ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
        return

    filtered = data_passenger[
        (data_passenger['ìš”ì¼êµ¬ë¶„']==selected_day) &
        (data_passenger['í˜¸ì„ ']==selected_line) &
        (data_passenger['ì¶œë°œì—­'].isin(selected_stations)) &
        (data_passenger['ìŠ¹í•˜ì°¨êµ¬ë¶„']==selected_ride)
    ].copy()
    filtered['ì¸ì›ìˆ˜'] = filtered['ì¸ì›ìˆ˜'].fillna(0).round(0).astype(int)

    if filtered.empty:
        st.warning("ì„ íƒí•œ ì¡°ê±´ì˜ ìŠ¹í•˜ì°¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    fig = px.bar(filtered, x='ì‹œê°„ëŒ€', y='ì¸ì›ìˆ˜', color='ì¶œë°œì—­', barmode='group',
             title=f"{selected_day} / {selected_line} ì‹œê°„ëŒ€ë³„ {selected_ride} ì¸ì› (ì¼í‰ê· )")
    fig.update_layout(xaxis_title="ì‹œê°„ëŒ€", yaxis_title="í‰ê·  ì¸ì›ìˆ˜ (ëª…)", xaxis={'type': 'category'})
    st.plotly_chart(fig, use_container_width=True)
    st.dataframe(filtered)


def show_ranking(data_ranking):
    """ìˆ˜ì†¡ ìˆœìœ„ í˜ì´ì§€"""
    st.header("ğŸ† ì§€í•˜ì²  ìˆ˜ì†¡ ìˆœìœ„ (2024ë…„ ê¸°ì¤€)")
     
    if 'í˜¸ì„ ' in data_ranking.columns:
        all_lines = list(data_ranking['í˜¸ì„ '].unique())
        all_lines.insert(0, "ì „ì²´ í˜¸ì„ ") 
        selected_line = st.selectbox("ì¡°íšŒí•  í˜¸ì„  ì„ íƒ", all_lines, key="rank_line")
        if selected_line == "ì „ì²´ í˜¸ì„ ":
            filtered_data = data_ranking
        else:
            filtered_data = data_ranking[data_ranking['í˜¸ì„ '] == selected_line]
        st.dataframe(filtered_data.reset_index(drop=True))
    else:
        st.dataframe(data_ranking)


# --- â¬‡ï¸â¬‡ï¸â¬‡ï¸ [ìˆ˜ì •ëœ í•¨ìˆ˜] â¬‡ï¸â¬‡ï¸â¬‡ï¸ ---

# [ìˆ˜ì •] ë¬´ê±°ìš´ í™˜ìŠ¹ í—ˆë¸Œ ê³„ì‚°ì„ ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ê³  ìºì‹œ ì ìš©
@st.cache_data
def calculate_transfer_hubs(data_passenger, data_bus):
    """
    Top 10 ì§€í•˜ì² ì—­ì˜ ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” (ë¬´ê±°ìš´) í•¨ìˆ˜
    """
    # 1. 'í‰ì¼' 'ìŠ¹ì°¨' ê¸°ì¤€ ì¼í‰ê·  ìŠ¹ê° Top 10 ì§€í•˜ì² ì—­ ì¶”ì¶œ
    passenger_filtered = data_passenger[
        (data_passenger['ìš”ì¼êµ¬ë¶„'] == 'í‰ì¼') &
        (data_passenger['ìŠ¹í•˜ì°¨êµ¬ë¶„'] == 'ìŠ¹ì°¨')
    ]
    passenger_sum = passenger_filtered.groupby('ì¶œë°œì—­')['ì¸ì›ìˆ˜'].sum().reset_index()
    top_10_stations = passenger_sum.sort_values(by='ì¸ì›ìˆ˜', ascending=False).head(10)
    top_10_stations['ì¸ì›ìˆ˜'] = top_10_stations['ì¸ì›ìˆ˜'].round(0).astype(int)
     
    # 2. Top 10 ì—­ì„ ìˆœíšŒí•˜ë©° ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜ ê³„ì‚°
    bus_route_counts = [] # ë²„ìŠ¤ ë…¸ì„  ìˆ˜ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸
     
    for station_name in top_10_stations['ì¶œë°œì—­']:
        # 'ê°•ë‚¨'ì—­ì˜ ê²½ìš°, 'ê°•ë‚¨ì—­', 'ê°•ë‚¨ì—­.12ë²ˆì¶œêµ¬' ë“±ì„ ëª¨ë‘ ì°¾ì•„ì•¼ í•¨
        relevant_stops = data_bus[data_bus['ì •ë¥˜ì†Œëª…'].str.contains(station_name, na=False)]
         
        # ì°¾ì€ ì •ë¥˜ì†Œë“¤ì˜ 'ë…¸ì„ ìˆ˜'ë¥¼ ëª¨ë‘ í•©ì‚°
        total_routes = relevant_stops['ë…¸ì„ ìˆ˜'].sum()
        bus_route_counts.append(total_routes)
         
    # 3. Top 10 ì§€í•˜ì² ì—­ ë°ì´í„°ì— 'ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜' ì—´ ì¶”ê°€
    top_10_stations['ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜'] = bus_route_counts
    top_10_stations['ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜'] = top_10_stations['ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜'].astype(int)
    
    return top_10_stations


def show_summary_analysis(data_congestion, data_passenger, data_ranking, data_bus):
    """
    [ìˆ˜ì •] 4ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ
    """
    st.header("ğŸ“Š ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.info("ì§€í•˜ì²  ì—­ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë³€ ì •ë¥˜ì¥ì˜ ë²„ìŠ¤ ë…¸ì„ ì„ ì§‘ê³„í•˜ì—¬ ê°€ëŠ¥í•œ ë…¸ì„  ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")

    # --- 1. ê¸°ì¡´ ì§€í•˜ì²  Top 5 ë¶„ì„ ---
    st.subheader("ğŸš‡ ì§€í•˜ì²  í•µì‹¬ í˜„í™© Top 5")
    col1, col2 = st.columns(2)
     
    with col1:
        try:
            st.markdown("#### ğŸ† 2024ë…„ ìˆ˜ì†¡ ìˆœìœ„")
            # [ìˆ˜ì •] ì´ ê³„ì‚°ì€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ ìºì‹œê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
            top_5_ranking = data_ranking.sort_values('ìˆœìœ„').head(5)
            st.dataframe(top_5_ranking)
        except Exception as e:
            st.error(f"ìˆ˜ì†¡ ìˆœìœ„ Top 5 ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            st.dataframe(data_ranking.head(5))

    with col2:
        try:
            st.markdown("#### ğŸ¥µ 2025ë…„ ìµœê³  í˜¼ì¡ë„")
            # [ìˆ˜ì •] ì´ ê³„ì‚°ì€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ ìºì‹œê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
            top_5_congestion = data_congestion.sort_values('í˜¼ì¡ë„', ascending=False).head(5)
            st.dataframe(top_5_congestion[['í˜¸ì„ ', 'ì¶œë°œì—­', 'ìƒí•˜êµ¬ë¶„', 'ì‹œê°„', 'í˜¼ì¡ë„']])
        except Exception as e:
            st.error(f"ìµœê³  í˜¼ì¡ë„ Top 5 ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
             
    # --- 2. [ì‹ ê·œ] í™˜ìŠ¹ í—ˆë¸Œ ë¶„ì„ ---
    st.subheader("ğŸšŒ ğŸ†š ğŸš‡ ì§€í•˜ì² -ë²„ìŠ¤ í™˜ìŠ¹ í—ˆë¸Œ ë¶„ì„ (Top 10)")
     
    try:
        # [ìˆ˜ì •] ìºì‹œëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê³„ì‚°ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ê²°ê³¼ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        top_10_hubs = calculate_transfer_hubs(data_passenger, data_bus)

        # 4. ê²°ê³¼ ì¶œë ¥
        st.dataframe(top_10_hubs)
        st.info(" 'ì—°ê³„ ë²„ìŠ¤ ë…¸ì„  ìˆ˜'ëŠ” í•´ë‹¹ ì§€í•˜ì² ì—­ ì´ë¦„ì´ í¬í•¨ëœ ì£¼ë³€ ë²„ìŠ¤ì •ë¥˜ì†Œì˜ ì´ ë…¸ì„  í•©ê³„ì…ë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"í™˜ìŠ¹ í—ˆë¸Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
# --- â¬†ï¸â¬†ï¸â¬†ï¸ [ìˆ˜ì •ëœ í•¨ìˆ˜] â¬†ï¸â¬†ï¸â¬†ï¸ ---


def show_bus_analysis(data_bus):
    """
    ë²„ìŠ¤ ì •ë¥˜ì†Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” í˜ì´ì§€
    """
    st.header("ğŸšŒ ë²„ìŠ¤ ì •ë¥˜ì†Œ ë¶„ì„ (2023ë…„ ê¸°ì¤€)")
    st.info("ì„œìš¸ì‹œ ë²„ìŠ¤ ì •ë¥˜ì†Œë³„ ë…¸ì„  ìˆ˜ í˜„í™©ì…ë‹ˆë‹¤.")
     
    try:
        # [ìˆ˜ì •] ì´ ê³„ì‚°ì€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ ìºì‹œê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
        top_bus_stops = data_bus.sort_values(by='ë…¸ì„ ìˆ˜', ascending=False)
         
        st.subheader("ğŸ“Š ë…¸ì„  ìˆ˜ ê¸°ì¤€ Top 20 ë²„ìŠ¤ ì •ë¥˜ì†Œ")
        st.dataframe(top_bus_stops[['ì •ë¥˜ì†Œëª…', 'ë…¸ì„ ìˆ˜', 'ì •ë¥˜ì†Œë²ˆí˜¸']].head(20))

        st.subheader("ğŸ” ì •ë¥˜ì†Œ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰")
        search_name = st.text_input("ê²€ìƒ‰í•  ì •ë¥˜ì†Œ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê°•ë‚¨ì—­)")
         
        if search_name: 
            search_result = top_bus_stops[top_bus_stops['ì •ë¥˜ì†Œëª…'].str.contains(search_name)]
            if search_result.empty:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.dataframe(search_result)
                 
    except Exception as e:
        st.error(f"ë²„ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        st.warning(" 'ë…¸ì„ ìˆ˜', 'ì •ë¥˜ì†Œëª…', 'ì •ë¥˜ì†Œë²ˆí˜¸' ì—´ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")


# --- 5. ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ---

# 1. íŒŒì¼ ì´ë¦„ ì •ì˜
file_congestion = "ì„œìš¸êµí†µê³µì‚¬_ì§€í•˜ì² í˜¼ì¡ë„ì •ë³´_20250630.csv"
file_passenger = "ì„œìš¸êµí†µê³µì‚¬_ì—­ë³„ ì¼ë³„ ì‹œê°„ëŒ€ë³„ ìŠ¹í•˜ì°¨ì¸ì› ì •ë³´_20231231.csv"
file_ranking = "ì„œìš¸êµí†µê³µì‚¬_ìˆ˜ì†¡ìˆœìœ„_20241231.csv"
file_bus_location = "ì„œìš¸ì‹œë²„ìŠ¤ì •ë¥˜ì†Œìœ„ì¹˜ì •ë³´(20251103).xlsx" 
file_bus_status = "ì„œìš¸ì‹œ ì •ë¥˜ì†Œí˜„í™©(2019~2023ë…„).xlsx" 

# 2. ë°ì´í„° ë¡œë“œ (ì´ì œ ì´ í•¨ìˆ˜ë“¤ì€ ìºì‹œëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤)
data_congestion = load_congestion_data(file_congestion)
data_passenger = load_passenger_data(file_passenger)
data_ranking = load_ranking_data(file_ranking)
data_bus = load_bus_data(file_bus_location, file_bus_status)

# 3. ë¡œë“œëœ ë°ì´í„° ê°œìˆ˜ í™•ì¸
data_list = [data_congestion, data_passenger, data_ranking, data_bus]
total_files = len(data_list) 

data_loaded_count = 0
for d in data_list:
    if d is not None and not d.empty:
        data_loaded_count = data_loaded_count + 1 

# 4. ì‚¬ì´ë“œë°” ë©”ë‰´
menu_options = [
    'HOME', 
    'ğŸ“Š ì¢…í•© ë¶„ì„ (í™˜ìŠ¹ í—ˆë¸Œ)',
    'ğŸšŒ ë²„ìŠ¤ ì •ë¥˜ì†Œ ë¶„ì„',
    'ğŸ† ìˆ˜ì†¡ ìˆœìœ„ (ì§€í•˜ì² )',
    'ğŸ“ˆ í˜¼ì¡ë„ ë¶„ì„ (ì§€í•˜ì² )',
    'ğŸ‘¥ ìŠ¹í•˜ì°¨ ë¶„ì„ (ì§€í•˜ì² )'
]
menu = st.sidebar.selectbox("ë©”ë‰´ ì„ íƒ", menu_options)

# 5. ë©”ë‰´ì— ë”°ë¼ ë‹¤ë¥¸ í˜ì´ì§€ ë³´ì—¬ì£¼ê¸°
if menu == 'HOME':
    show_home(data_loaded_count, total_files)

elif menu == 'ğŸ“Š ì¢…í•© ë¶„ì„ (í™˜ìŠ¹ í—ˆë¸Œ)':
    if all(d is not None and not d.empty for d in data_list):
        show_summary_analysis(data_congestion, data_passenger, data_ranking, data_bus)
    else:
        st.error("ì¢…í•© ë¶„ì„ì— í•„ìš”í•œ 4ê°œ ë°ì´í„°ê°€ ëª¨ë‘ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

elif menu == 'ğŸšŒ ë²„ìŠ¤ ì •ë¥˜ì†Œ ë¶„ì„':
    if data_bus is not None and not data_bus.empty:
        show_bus_analysis(data_bus)
    else:
        st.error("ë²„ìŠ¤ ì •ë¥˜ì†Œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
         
elif menu == 'ğŸ† ìˆ˜ì†¡ ìˆœìœ„ (ì§€í•˜ì² )':
    if data_ranking is not None and not data_ranking.empty:
        show_ranking(data_ranking)
    else:
        st.error("ìˆ˜ì†¡ ìˆœìœ„ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

elif menu == 'ğŸ“ˆ í˜¼ì¡ë„ ë¶„ì„ (ì§€í•˜ì² )':
    if data_congestion is not None and not data_congestion.empty and \
       data_passenger is not None and not data_passenger.empty:
        show_congestion_analysis(data_congestion, data_passenger)
    else:
        st.error("í˜¼ì¡ë„ ë˜ëŠ” ìŠ¹í•˜ì°¨ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

elif menu == 'ğŸ‘¥ ìŠ¹í•˜ì°¨ ë¶„ì„ (ì§€í•˜ì² )':
    if data_congestion is not None and not data_congestion.empty and \
       data_passenger is not None and not data_passenger.empty:
        show_passenger_analysis(data_congestion, data_passenger)
    else:
        st.error("í˜¼ì¡ë„ ë˜ëŠ” ìŠ¹í•˜ì°¨ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")